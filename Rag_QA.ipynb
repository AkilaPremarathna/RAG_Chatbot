{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Akila Personal\\Projects\\CV_Project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from dotenv import load_dotenv\n",
    "from pypdf import PdfReader\n",
    "import google.generativeai as genai\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "import uuid\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def load_pdf(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads text content from a PDF file and returns it as a single string.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the PDF file.\n",
    "    \n",
    "    Returns:\n",
    "        str: Concatenated text from all pages.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() or \"\"  # Handle cases where extract_text returns None\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    \"\"\"\n",
    "    Custom embedding function using the Gemini AI API for document retrieval.\n",
    "    \"\"\"\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "        if not gemini_api_key:\n",
    "            raise ValueError(\"Gemini API Key not provided. Please provide GEMINI_API_KEY as an environment variable\")\n",
    "        genai.configure(api_key=gemini_api_key)\n",
    "        model = \"models/embedding-001\"\n",
    "        title = \"Custom query\"\n",
    "        return genai.embed_content(model=model,\n",
    "                                   content=input,\n",
    "                                   task_type=\"retrieval_document\",\n",
    "                                   title=title)[\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chroma_db(documents: List[str], path: str, name: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Creates a Chroma database with the provided documents.\n",
    "    \n",
    "    Args:\n",
    "        documents (List[str]): List of text chunks to embed.\n",
    "        path (str): Directory path for ChromaDB persistence.\n",
    "        name (str): Name of the Chroma collection.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Chroma collection object and its name.\n",
    "    \"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=path)\n",
    "    db = chroma_client.create_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
    "    for i, d in enumerate(documents):\n",
    "        db.add(documents=[d], ids=str(i))\n",
    "    return db, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chroma_collection(path: str, name: str):\n",
    "    \"\"\"\n",
    "    Loads an existing Chroma collection.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Directory path of ChromaDB.\n",
    "        name (str): Name of the collection to load.\n",
    "    \n",
    "    Returns:\n",
    "        chromadb.Collection: Loaded Chroma collection.\n",
    "    \"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=path)\n",
    "    db = chroma_client.get_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(query: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the query using Gemini AI with task_type=\"retrieval_query\".\n",
    "    \n",
    "    Args:\n",
    "        query (str): The query text.\n",
    "    \n",
    "    Returns:\n",
    "        List[float]: The embedded query vector.\n",
    "    \"\"\"\n",
    "    gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not gemini_api_key:\n",
    "        raise ValueError(\"Gemini API Key not provided.\")\n",
    "    genai.configure(api_key=gemini_api_key)\n",
    "    model = \"models/embedding-001\"\n",
    "    return genai.embed_content(model=model,\n",
    "                               content=query,\n",
    "                               task_type=\"retrieval_query\")[\"embedding\"]\n",
    "\n",
    "def get_relevant_passage(query: str, db, n_results: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieves the most relevant text chunks from ChromaDB based on the query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User's question.\n",
    "        db: Chroma collection object.\n",
    "        n_results (int): Number of top results to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: List of relevant text chunks.\n",
    "    \"\"\"\n",
    "    query_embedding = embed_query(query)\n",
    "    results = db.query(query_embeddings=[query_embedding], n_results=n_results)\n",
    "    return results['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rag_prompt(query: str, relevant_passage: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates a prompt for the Gemini model using the query and relevant text.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User's question.\n",
    "        relevant_passage (str): Retrieved text to include in the prompt.\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted prompt string.\n",
    "    \"\"\"\n",
    "    escaped = relevant_passage.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n",
    "    prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage below. \n",
    "    Respond in complete sentences, be comprehensive, and include all relevant background information. \n",
    "    Since the user may not know the context, break down complicated concepts and use a friendly, conversational tone. \n",
    "    If the passage doesn’t contain enough information to answer the question, say you don’t have enough info to provide a full answer.\n",
    "    QUESTION: '{query}'\n",
    "    PASSAGE: '{escaped}'\n",
    "\n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_answer(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates an answer using the Gemini AI model.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): Formatted prompt string.\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated answer text.\n",
    "    \"\"\"\n",
    "    gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not gemini_api_key:\n",
    "        raise ValueError(\"Gemini API Key not provided. Please provide GEMINI_API_KEY as an environment variable\")\n",
    "    genai.configure(api_key=gemini_api_key)\n",
    "    model = genai.GenerativeModel('gemini-2.0-pro-exp-02-05')\n",
    "    answer = model.generate_content(prompt)\n",
    "    return answer.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(db, query: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates an answer to the query using the RAG pipeline.\n",
    "    \n",
    "    Args:\n",
    "        db: Chroma collection object.\n",
    "        query (str): User's question.\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated answer.\n",
    "    \"\"\"\n",
    "    relevant_text = get_relevant_passage(query, db, n_results=3)\n",
    "    prompt = make_rag_prompt(query, \" \".join(relevant_text))\n",
    "    answer = gemini_answer(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF...\n",
      "Splitting text into chunks...\n",
      "Created 3 chunks.\n",
      "\n",
      "Querying: 'What are the projct that he has done?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 0, updating n_results = 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "That's a great question! You're asking about the projects that a particular person has completed.\n",
      "\n",
      "However, the provided text passage is completely empty. Because there's no information in the passage, I can't tell you who \"he\" refers to or what projects that person might have worked on.\n",
      "\n",
      "To answer your question, I'd need a passage that mentions a specific person and describes the projects they have been involved with. Sorry I couldn't be more helpful with the text given!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Define file path and database parameters\n",
    "    file_path = r\"Resume2.pdf\"\n",
    "    db_path = r\"vectordb\"\n",
    "    collection_name = \"rag_experiment_v5\"\n",
    "\n",
    "    # Step 1: Read and scrape text from PDF\n",
    "    print(\"Loading PDF...\")\n",
    "    pdf_text = load_pdf(file_path)\n",
    "\n",
    "    # Step 2: Chunk the text using RecursiveCharacterTextSplitter\n",
    "    print(\"Splitting text into chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  # Adjusted for meaningful chunks\n",
    "        chunk_overlap=100,  # Overlap to maintain context\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunked_text = text_splitter.split_text(pdf_text)\n",
    "    chunked_text = [chunk for chunk in chunked_text if len(chunk.strip()) > 50]\n",
    "    print(f\"Created {len(chunked_text)} chunks.\")\n",
    "\n",
    "    \n",
    "    # # Step 3: Embed and store in ChromaDB (uncomment to create anew)\n",
    "    print(\"Creating new ChromaDB collection...\")\n",
    "    db, name = create_chroma_db(documents=chunked_text, path=db_path, name=collection_name)\n",
    "\n",
    "    # Step 4: Load the collection\n",
    "    db = load_chroma_collection(path=db_path, name=collection_name)\n",
    "\n",
    "        #   Step 4: Delete all existing documents in the collection\n",
    "    # all_ids = db.get()['ids']\n",
    "    # if all_ids:\n",
    "    #     db.delete(ids=all_ids)\n",
    "    #     print(f\"Deleted {len(all_ids)} existing documents from the collection.\")\n",
    "\n",
    "\n",
    "\n",
    "    # Step 5: Query the system\n",
    "    test_query = \"What are the projct that he has done?\"\n",
    "    print(f\"\\nQuerying: '{test_query}'\")\n",
    "    answer = generate_answer(db, test_query)\n",
    "    print(\"Answer:\")\n",
    "    print(answer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF...\n",
      "Splitting text into chunks...\n",
      "Created 7 chunks.\n",
      "Loading ChromaDB collection...\n",
      "Deleted 7 existing documents from the collection.\n",
      "Adding new documents to ChromaDB...\n",
      "Added 7 new documents to the collection.\n",
      "\n",
      "Querying: 'what are the experience he has?'\n",
      "Answer:\n",
      "Based on the passage, this individual has a diverse range of experiences, particularly blending surveying science, data collection, and cutting-edge AI development. Let's break it down:\n",
      "\n",
      "1.  **Data Collection and Geospatial Analysis:**\n",
      "    *   **Hands-on Field Experience:** They have practical experience collecting data using several advanced technologies:\n",
      "        *   **UAV LIDAR:** This involves using drones (Unmanned Aerial Vehicles) equipped with Light Detection and Ranging (LIDAR) sensors to create detailed 3D maps of the ground surface.\n",
      "        *   **TLS (Terrestrial Laser Scanner):** This is ground-based laser scanning, also used to create highly accurate 3D models (point clouds) of objects and environments. They specifically mention extracting TLS point cloud data using Python with Dr. G.S.N Perera.\n",
      "        *   **GPR (Ground Penetration Radar):** This technique uses radar pulses to image the subsurface, often used in geology, archaeology, or utility mapping.\n",
      "        *   **GNSS (Global Navigation Satellite System):** This is the standard technology for location positioning (like GPS). They used it in a project to compare the vertical accuracy (Z component) of UAV LIDAR data.\n",
      "    *   **Earth Observation Data Processing:** In their role at IWMI, they are creating automated scripts using Google Earth Engine (a cloud platform for planetary-scale environmental data analysis) and Google Collab (an online coding environment) to extract building footprint coordinates. This involves using datasets like the ESRI LULC (Land Use Land Cover) 10m dataset to identify built-up areas.\n",
      "\n",
      "2.  **AI and Software Development:**\n",
      "    *   **AI-Driven CV Ranking System:** They developed a sophisticated system to automatically rank resumes (CVs). This involved:\n",
      "        *   **Text Extraction:** Using tools like `pdfplumber` and `pytesseract` OCR (Optical Character Recognition) to pull text even from scanned PDF resumes.\n",
      "        *   **Generative AI:** Leveraging the Gemini API (a large language model from Google) and LangChain (a framework for developing applications powered by language models) for tasks like extracting specific skills and calculating relevant experience based on job roles. They designed specific instructions (\"prompts\") to get structured information (in JSON format) from the AI.\n",
      "        *   **Database Integration:** Using MongoDB (a NoSQL database) to efficiently store and retrieve job requirements.\n",
      "        *   **Algorithm Development:** Creating custom algorithms to calculate skill match percentages and experience scores.\n",
      "        *   **GUI Development:** Building an interactive user interface using Tkinter (a Python library for creating graphical user interfaces or GUIs), which included features like a searchable job list and a chatbot powered by Gemini for querying CVs.\n",
      "    *   **AI-Powered PDF Parser using RAG:** They built another AI system specifically for parsing information from resume PDFs. This used a technique called Retrieval-Augmented Generation (RAG), which combines retrieving relevant information with AI text generation. Key components included:\n",
      "        *   **PDF Processing:** Using `PyPDF` for text extraction and LangChain tools for breaking text into manageable chunks.\n",
      "        *   **Semantic Search:** Using the Gemini API to create \"semantic embeddings\" (numerical representations of meaning) and managing these in ChromaDB (a vector database) to efficiently find relevant text passages within the resumes.\n",
      "        *   **Structured Data Extraction:** Designing prompts to guide the Gemini model in extracting specific information (like name, skills, education) into a structured JSON format.\n",
      "    *   **Model Testing:** They have experience testing different AI models, including Google's Gemini and Gemma Large Language Models (LLMs) for identifying wastewater treatment plants, and various versions of the YOLO model (an object detection algorithm) for object identification tasks.\n",
      "\n",
      "3.  **Research and Collaboration:**\n",
      "    *   **Literature Analysis:** They have experience conducting research literature analysis.\n",
      "    *   **Collaboration & Communication:** They are skilled in presenting insights (\"crafting and presenting daily insights\"), organizing meetings (using Microsoft Teams), and working effectively with international researchers.\n",
      "\n",
      "4.  **Formal Work Experience:**\n",
      "    *   **Research Consultant - Data Scientist:** Currently employed at IWMI (International Water Management Institute) since May 2024, focusing on the geospatial data extraction tasks mentioned earlier.\n",
      "\n",
      "In summary, this individual possesses a strong combination of hands-on experience in geospatial data collection (LIDAR, TLS, GPR, GNSS), data processing (Python, Google Earth Engine), and advanced AI development skills (NLP, Generative AI like Gemini, RAG, machine learning models like YOLO, GUI development, database management) applied to practical problems like recruitment and environmental analysis. They also have experience in research and collaborative project environments.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Define file path and database parameters\n",
    "    file_path = r\"CV - Akila  Nishan .pdf\"\n",
    "    db_path = r\"vectordb\"\n",
    "    collection_name = \"rag_experiment_v3\"\n",
    "\n",
    "    # Step 1: Read and scrape text from PDF\n",
    "    print(\"Loading PDF...\")\n",
    "    pdf_text = load_pdf(file_path)\n",
    "\n",
    "    # Step 2: Chunk the text using RecursiveCharacterTextSplitter\n",
    "    print(\"Splitting text into chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  # Adjusted for meaningful chunks\n",
    "        chunk_overlap=100,  # Overlap to maintain context\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunked_text = text_splitter.split_text(pdf_text)\n",
    "    chunked_text = [chunk for chunk in chunked_text if len(chunk.strip()) > 50]\n",
    "    print(f\"Created {len(chunked_text)} chunks.\")\n",
    "\n",
    "    #   # # Step 3: Embed and store in ChromaDB (uncomment to create anew)\n",
    "    # print(\"Creating new ChromaDB collection...\")\n",
    "    # db, name = create_chroma_db(documents=chunked_text, path=db_path, name=collection_name)\n",
    "\n",
    "\n",
    "    # Step 3: Load the ChromaDB collection\n",
    "    print(\"Loading ChromaDB collection...\")\n",
    "    db = load_chroma_collection(path=db_path, name=collection_name)\n",
    "\n",
    "    # Step 4: Delete all existing documents in the collection\n",
    "    all_ids = db.get()['ids']\n",
    "    if all_ids:\n",
    "        db.delete(ids=all_ids)\n",
    "        print(f\"Deleted {len(all_ids)} existing documents from the collection.\")\n",
    "\n",
    "    # Step 5: Add new documents to the collection\n",
    "    print(\"Adding new documents to ChromaDB...\")\n",
    "    for i, chunk in enumerate(chunked_text):\n",
    "        db.add(documents=[chunk], ids=[str(i)])\n",
    "    print(f\"Added {len(chunked_text)} new documents to the collection.\")\n",
    "\n",
    "    # Step 6: Query the system\n",
    "    test_query = \"what are the experience he has?\"\n",
    "    print(f\"\\nQuerying: '{test_query}'\")\n",
    "    answer = generate_answer(db, test_query)\n",
    "    print(\"Answer:\")\n",
    "    print(answer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
