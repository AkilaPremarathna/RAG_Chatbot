{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from dotenv import load_dotenv\n",
    "from pypdf import PdfReader\n",
    "import google.generativeai as genai\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "import uuid\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from typing import Dict, List, Tuple\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# def load_pdf(file_path: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Reads text content from a PDF file and returns it as a single string.\n",
    "    \n",
    "#     Args:\n",
    "#         file_path (str): Path to the PDF file.\n",
    "    \n",
    "#     Returns:\n",
    "#         str: Concatenated text from all pages.\n",
    "#     \"\"\"\n",
    "#     reader = PdfReader(file_path)\n",
    "#     text = \"\"\n",
    "#     for page in reader.pages:\n",
    "#         text += page.extract_text() or \"\"  # Handle cases where extract_text returns None\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    \"\"\"\n",
    "    Custom embedding function using the Gemini AI API for document retrieval.\n",
    "    \"\"\"\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "        if not gemini_api_key:\n",
    "            raise ValueError(\"Gemini API Key not provided. Please provide GEMINI_API_KEY as an environment variable\")\n",
    "        genai.configure(api_key=gemini_api_key)\n",
    "        model = \"models/embedding-001\"\n",
    "        title = \"Custom query\"\n",
    "        return genai.embed_content(model=model,\n",
    "                                   content=input,\n",
    "                                   task_type=\"retrieval_document\",\n",
    "                                   title=title)[\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chroma_db(documents: List[str], path: str, name: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Creates a Chroma database with the provided documents.\n",
    "    \n",
    "    Args:\n",
    "        documents (List[str]): List of text chunks to embed.\n",
    "        path (str): Directory path for ChromaDB persistence.\n",
    "        name (str): Name of the Chroma collection.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Chroma collection object and its name.\n",
    "    \"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=path)\n",
    "    db = chroma_client.create_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
    "    for i, d in enumerate(documents):\n",
    "        db.add(documents=[d], ids=str(i))\n",
    "    return db, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chroma_collection(path: str, name: str):\n",
    "    \"\"\"\n",
    "    Loads an existing Chroma collection.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Directory path of ChromaDB.\n",
    "        name (str): Name of the collection to load.\n",
    "    \n",
    "    Returns:\n",
    "        chromadb.Collection: Loaded Chroma collection.\n",
    "    \"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=path)\n",
    "    db = chroma_client.get_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "def clean_pdf(pdf_path: str, min_word_count: int = 20) -> Tuple[str, Dict]:\n",
    "    \"\"\"\n",
    "    Reads a PDF file, removes pages with fewer than the specified word count,\n",
    "    and returns the cleaned content along with metadata.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        min_word_count (int): Minimum word count for a page to be included (default: 20)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[str, Dict]: A tuple containing:\n",
    "            - cleaned_content (str): The cleaned content from the PDF\n",
    "            - metadata (Dict): Metadata about the original and cleaned PDF\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"PDF file not found at: {pdf_path}\")\n",
    "    \n",
    "    # Load the PDF\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = loader.load_and_split()\n",
    "    \n",
    "    # Prepare variables for metadata\n",
    "    total_pages = len(pages)\n",
    "    removed_pages = []\n",
    "    kept_pages = []\n",
    "    cleaned_content = \"\"\n",
    "    \n",
    "    # Process each page\n",
    "    for i, page in enumerate(pages):\n",
    "        page_content = page.page_content.strip()\n",
    "        words = re.findall(r'\\b\\w+\\b', page_content)\n",
    "        word_count = len(words)\n",
    "        \n",
    "        # Decide whether to keep or remove the page\n",
    "        if word_count >= min_word_count:\n",
    "            cleaned_content += page_content + \"\\n\\n\"\n",
    "            kept_pages.append(i + 1)  # +1 for human-readable page numbers\n",
    "        else:\n",
    "            removed_pages.append(i + 1)  # +1 for human-readable page numbers\n",
    "    return cleaned_content\n",
    "\n",
    "# Example usage in Jupyter notebook:\n",
    "# Replace 'your_pdf_file.pdf' with the path to your PDF file\n",
    "# pdf_content = clean_pdf(r'pdfs\\2312.10997v5.pdf')\n",
    "# # \n",
    "# # # Access the cleaned content\n",
    "# print(\"Cleaned Content Preview (first 500 chars):\")\n",
    "# print(pdf_content[:50000] + \"...\" if len(pdf_content) > 500 else pdf_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(query: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the query using Gemini AI with task_type=\"retrieval_query\".\n",
    "    \n",
    "    Args:\n",
    "        query (str): The query text.\n",
    "    \n",
    "    Returns:\n",
    "        List[float]: The embedded query vector.\n",
    "    \"\"\"\n",
    "    gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not gemini_api_key:\n",
    "        raise ValueError(\"Gemini API Key not provided.\")\n",
    "    genai.configure(api_key=gemini_api_key)\n",
    "    model = \"models/embedding-001\"\n",
    "    return genai.embed_content(model=model,\n",
    "                               content=query,\n",
    "                               task_type=\"retrieval_query\")[\"embedding\"]\n",
    "\n",
    "def get_relevant_passage(query: str, db, n_results: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieves the most relevant text chunks from ChromaDB based on the query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User's question.\n",
    "        db: Chroma collection object.\n",
    "        n_results (int): Number of top results to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: List of relevant text chunks.\n",
    "    \"\"\"\n",
    "    query_embedding = embed_query(query)\n",
    "    results = db.query(query_embeddings=[query_embedding], n_results=n_results)\n",
    "    return results['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rag_prompt(query: str, relevant_passage: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates a prompt for the Gemini model using the query and relevant text.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User's question.\n",
    "        relevant_passage (str): Retrieved text to include in the prompt.\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted prompt string.\n",
    "    \"\"\"\n",
    "    escaped = relevant_passage.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n",
    "    prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage below. \n",
    "    Respond in complete sentences, be comprehensive, and include all relevant background information. \n",
    "    dont write hufe number of words, simply give the answer to the question.\n",
    "    Since the user may not know the context, break down complicated concepts and use a friendly, conversational tone. \n",
    "    If the passage doesn’t contain enough information to answer the question, say you don’t have enough info to provide a full answer.\n",
    "    QUESTION: '{query}'\n",
    "    PASSAGE: '{escaped}'\n",
    "\n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_answer(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates an answer using the Gemini AI model.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): Formatted prompt string.\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated answer text.\n",
    "    \"\"\"\n",
    "    gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not gemini_api_key:\n",
    "        raise ValueError(\"Gemini API Key not provided. Please provide GEMINI_API_KEY as an environment variable\")\n",
    "    genai.configure(api_key=gemini_api_key)\n",
    "    model = genai.GenerativeModel('gemini-2.0-pro-exp-02-05')\n",
    "    answer = model.generate_content(prompt)\n",
    "    return answer.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(db, query: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates an answer to the query using the RAG pipeline.\n",
    "    \n",
    "    Args:\n",
    "        db: Chroma collection object.\n",
    "        query (str): User's question.\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated answer.\n",
    "    \"\"\"\n",
    "    relevant_text = get_relevant_passage(query, db, n_results=3)\n",
    "    prompt = make_rag_prompt(query, \" \".join(relevant_text))\n",
    "    answer = gemini_answer(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chroma_db_if_not_exists(documents, path, name):\n",
    "    collection_path = os.path.join(path, name)\n",
    "    if os.path.exists(collection_path):\n",
    "        print(f\"Collection [{name}] already exists. Skipping creation.\")\n",
    "        return None  # or return existing collection if you load it elsewhere\n",
    "    else:\n",
    "        return create_chroma_db(documents=documents, path=path, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF...\n",
      "Splitting text into chunks...\n",
      "Created 143 chunks.\n",
      "Creating new ChromaDB collection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.premarathna\\AppData\\Local\\Temp\\ipykernel_47340\\2068611281.py:14: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
      "  db = chroma_client.create_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Collection [rag_experiment_v5] already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mprint\u001b[39m(answer)\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# # Step 3: Embed and store in ChromaDB (uncomment to create anew)\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating new ChromaDB collection...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m db, name = \u001b[43mcreate_chroma_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdb_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Step 4: Load the collection\u001b[39;00m\n\u001b[32m     32\u001b[39m db = load_chroma_collection(path=db_path, name=collection_name)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mcreate_chroma_db\u001b[39m\u001b[34m(documents, path, name)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mCreates a Chroma database with the provided documents.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m \u001b[33;03m    tuple: Chroma collection object and its name.\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     13\u001b[39m chroma_client = chromadb.PersistentClient(path=path)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m db = \u001b[43mchroma_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGeminiEmbeddingFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(documents):\n\u001b[32m     16\u001b[39m     db.add(documents=[d], ids=\u001b[38;5;28mstr\u001b[39m(i))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Akila Personal\\Projects\\RAG_Chatbot\\.vemv\\Lib\\site-packages\\chromadb\\api\\client.py:156\u001b[39m, in \u001b[36mClient.create_collection\u001b[39m\u001b[34m(self, name, configuration, metadata, embedding_function, data_loader, get_or_create)\u001b[39m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    155\u001b[39m         configuration[\u001b[33m\"\u001b[39m\u001b[33membedding_function\u001b[39m\u001b[33m\"\u001b[39m] = embedding_function\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_server\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_or_create\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_or_create\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Collection(\n\u001b[32m    165\u001b[39m     client=\u001b[38;5;28mself\u001b[39m._server,\n\u001b[32m    166\u001b[39m     model=model,\n\u001b[32m    167\u001b[39m     embedding_function=embedding_function,\n\u001b[32m    168\u001b[39m     data_loader=data_loader,\n\u001b[32m    169\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Akila Personal\\Projects\\RAG_Chatbot\\.vemv\\Lib\\site-packages\\chromadb\\api\\rust.py:230\u001b[39m, in \u001b[36mRustBindingsAPI.create_collection\u001b[39m\u001b[34m(self, name, configuration, metadata, get_or_create, tenant, database)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    228\u001b[39m     configuration_json_str = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m collection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfiguration_json_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_or_create\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m collection_model = CollectionModel(\n\u001b[32m    234\u001b[39m     \u001b[38;5;28mid\u001b[39m=collection.id,\n\u001b[32m    235\u001b[39m     name=collection.name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    242\u001b[39m     database=collection.database,\n\u001b[32m    243\u001b[39m )\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m collection_model\n",
      "\u001b[31mInternalError\u001b[39m: Collection [rag_experiment_v5] already exists"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Define file path and database parameters\n",
    "   \n",
    "    # file_path = r\"Resume2.pdf\"\n",
    "    db_path = r\"vectordb\"\n",
    "    collection_name = \"rag_experiment_v5\"\n",
    "\n",
    "    # Step 1: Read and scrape text from PDF\n",
    "    print(\"Loading PDF...\")\n",
    "\n",
    "    # pdf_text = load_pdf(file_path)\n",
    "    pdf_text = clean_pdf(r'pdfs\\2312.10997v5.pdf')\n",
    "\n",
    "    # Step 2: Chunk the text using RecursiveCharacterTextSplitter\n",
    "    print(\"Splitting text into chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  # Adjusted for meaningful chunks\n",
    "        chunk_overlap=100,  # Overlap to maintain context\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunked_text = text_splitter.split_text(pdf_text)\n",
    "    chunked_text = [chunk for chunk in chunked_text if len(chunk.strip()) > 50]\n",
    "    print(f\"Created {len(chunked_text)} chunks.\")\n",
    "\n",
    "    \n",
    "    # # Step 3: Embed and store in ChromaDB (uncomment to create anew)\n",
    "    print(\"Creating new ChromaDB collection...\")\n",
    "    db, name = create_chroma_db(documents=chunked_text, path=db_path, name=collection_name)\n",
    "    \n",
    "   \n",
    "    # Step 4: Load the collection\n",
    "    db = load_chroma_collection(path=db_path, name=collection_name)\n",
    "\n",
    "        #   Step 4: Delete all existing documents in the collection\n",
    "    all_ids = db.get()['ids']\n",
    "    if all_ids:\n",
    "        db.delete(ids=all_ids)\n",
    "        print(f\"Deleted {len(all_ids)} existing documents from the collection.\")\n",
    "\n",
    "\n",
    "\n",
    "    # Step 5: Query the system\n",
    "    test_query = \"list me the authors of this paper?\"\n",
    "    print(f\"\\nQuerying: '{test_query}'\")\n",
    "    answer = generate_answer(db, test_query)\n",
    "    print(\"Answer:\")\n",
    "    print(answer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".vemv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
