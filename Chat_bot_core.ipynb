{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from dotenv import load_dotenv\n",
    "from pypdf import PdfReader\n",
    "import google.generativeai as genai\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "import uuid\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from typing import Dict, List, Tuple\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# def load_pdf(file_path: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Reads text content from a PDF file and returns it as a single string.\n",
    "    \n",
    "#     Args:\n",
    "#         file_path (str): Path to the PDF file.\n",
    "    \n",
    "#     Returns:\n",
    "#         str: Concatenated text from all pages.\n",
    "#     \"\"\"\n",
    "#     reader = PdfReader(file_path)\n",
    "#     text = \"\"\n",
    "#     for page in reader.pages:\n",
    "#         text += page.extract_text() or \"\"  # Handle cases where extract_text returns None\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    \"\"\"\n",
    "    Custom embedding function using the Gemini AI API for document retrieval.\n",
    "    \"\"\"\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "        if not gemini_api_key:\n",
    "            raise ValueError(\"Gemini API Key not provided. Please provide GEMINI_API_KEY as an environment variable\")\n",
    "        genai.configure(api_key=gemini_api_key)\n",
    "        model = \"models/embedding-001\"\n",
    "        title = \"Custom query\"\n",
    "        return genai.embed_content(model=model,\n",
    "                                   content=input,\n",
    "                                   task_type=\"retrieval_document\",\n",
    "                                   title=title)[\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chroma_db(documents: List[str], path: str, name: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Creates a Chroma database with the provided documents.\n",
    "    \n",
    "    Args:\n",
    "        documents (List[str]): List of text chunks to embed.\n",
    "        path (str): Directory path for ChromaDB persistence.\n",
    "        name (str): Name of the Chroma collection.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Chroma collection object and its name.\n",
    "    \"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=path)\n",
    "    db = chroma_client.create_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
    "    for i, d in enumerate(documents):\n",
    "        db.add(documents=[d], ids=str(i))\n",
    "    return db, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chroma_collection(path: str, name: str):\n",
    "    \"\"\"\n",
    "    Loads an existing Chroma collection.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Directory path of ChromaDB.\n",
    "        name (str): Name of the collection to load.\n",
    "    \n",
    "    Returns:\n",
    "        chromadb.Collection: Loaded Chroma collection.\n",
    "    \"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=path)\n",
    "    db = chroma_client.get_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "def clean_pdf(pdf_path: str, min_word_count: int = 20) -> Tuple[str, Dict]:\n",
    "    \"\"\"\n",
    "    Reads a PDF file, removes pages with fewer than the specified word count,\n",
    "    and returns the cleaned content along with metadata.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        min_word_count (int): Minimum word count for a page to be included (default: 20)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[str, Dict]: A tuple containing:\n",
    "            - cleaned_content (str): The cleaned content from the PDF\n",
    "            - metadata (Dict): Metadata about the original and cleaned PDF\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"PDF file not found at: {pdf_path}\")\n",
    "    \n",
    "    # Load the PDF\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = loader.load_and_split()\n",
    "    \n",
    "    # Prepare variables for metadata\n",
    "    total_pages = len(pages)\n",
    "    removed_pages = []\n",
    "    kept_pages = []\n",
    "    cleaned_content = \"\"\n",
    "    \n",
    "    # Process each page\n",
    "    for i, page in enumerate(pages):\n",
    "        page_content = page.page_content.strip()\n",
    "        words = re.findall(r'\\b\\w+\\b', page_content)\n",
    "        word_count = len(words)\n",
    "        \n",
    "        # Decide whether to keep or remove the page\n",
    "        if word_count >= min_word_count:\n",
    "            cleaned_content += page_content + \"\\n\\n\"\n",
    "            kept_pages.append(i + 1)  # +1 for human-readable page numbers\n",
    "        else:\n",
    "            removed_pages.append(i + 1)  # +1 for human-readable page numbers\n",
    "    return cleaned_content\n",
    "\n",
    "# Example usage in Jupyter notebook:\n",
    "# Replace 'your_pdf_file.pdf' with the path to your PDF file\n",
    "# pdf_content = clean_pdf(r'pdfs\\2312.10997v5.pdf')\n",
    "# # \n",
    "# # # Access the cleaned content\n",
    "# print(\"Cleaned Content Preview (first 500 chars):\")\n",
    "# print(pdf_content[:50000] + \"...\" if len(pdf_content) > 500 else pdf_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(query: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the query using Gemini AI with task_type=\"retrieval_query\".\n",
    "    \n",
    "    Args:\n",
    "        query (str): The query text.\n",
    "    \n",
    "    Returns:\n",
    "        List[float]: The embedded query vector.\n",
    "    \"\"\"\n",
    "    gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not gemini_api_key:\n",
    "        raise ValueError(\"Gemini API Key not provided.\")\n",
    "    genai.configure(api_key=gemini_api_key)\n",
    "    model = \"models/embedding-001\"\n",
    "    return genai.embed_content(model=model,\n",
    "                               content=query,\n",
    "                               task_type=\"retrieval_query\")[\"embedding\"]\n",
    "\n",
    "def get_relevant_passage(query: str, db, n_results: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retrieves the most relevant text chunks from ChromaDB based on the query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User's question.\n",
    "        db: Chroma collection object.\n",
    "        n_results (int): Number of top results to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: List of relevant text chunks.\n",
    "    \"\"\"\n",
    "    query_embedding = embed_query(query)\n",
    "    results = db.query(query_embeddings=[query_embedding], n_results=n_results)\n",
    "    return results['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rag_prompt(query: str, relevant_passage: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates a prompt for the Gemini model using the query and relevant text.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User's question.\n",
    "        relevant_passage (str): Retrieved text to include in the prompt.\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted prompt string.\n",
    "    \"\"\"\n",
    "    escaped = relevant_passage.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n",
    "    prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage below. \n",
    "    Respond in complete sentences, be comprehensive, and include all relevant background information. \n",
    "    dont write hufe number of words, simply give the answer to the question.\n",
    "    Since the user may not know the context, break down complicated concepts and use a friendly, conversational tone. \n",
    "    If the passage doesn’t contain enough information to answer the question, say you don’t have enough info to provide a full answer.\n",
    "    QUESTION: '{query}'\n",
    "    PASSAGE: '{escaped}'\n",
    "\n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_answer(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates an answer using the Gemini AI model.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): Formatted prompt string.\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated answer text.\n",
    "    \"\"\"\n",
    "    gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not gemini_api_key:\n",
    "        raise ValueError(\"Gemini API Key not provided. Please provide GEMINI_API_KEY as an environment variable\")\n",
    "    genai.configure(api_key=gemini_api_key)\n",
    "    model = genai.GenerativeModel('gemini-2.0-pro-exp-02-05')\n",
    "    answer = model.generate_content(prompt)\n",
    "    return answer.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(db, query: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates an answer to the query using the RAG pipeline.\n",
    "    \n",
    "    Args:\n",
    "        db: Chroma collection object.\n",
    "        query (str): User's question.\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated answer.\n",
    "    \"\"\"\n",
    "    relevant_text = get_relevant_passage(query, db, n_results=3)\n",
    "    prompt = make_rag_prompt(query, \" \".join(relevant_text))\n",
    "    answer = gemini_answer(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chroma_db_if_not_exists(documents, path, name):\n",
    "    collection_path = os.path.join(path, name)\n",
    "    if os.path.exists(collection_path):\n",
    "        print(f\"Collection [{name}] already exists. Skipping creation.\")\n",
    "        return None  # or return existing collection if you load it elsewhere\n",
    "    else:\n",
    "        return create_chroma_db(documents=documents, path=path, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Define file path and database parameters\n",
    "    db_path = r\"vectordb\"\n",
    "    collection_name = \"rag_experiment_v5\"\n",
    "\n",
    "    # Step 1: Read and scrape text from PDF\n",
    "    print(\"Loading PDF...\")\n",
    "    pdf_text = clean_pdf(r'pdfs\\Premarathna, Akila Nishan - 4500013458 - Cost Amendment.docx.pdf')\n",
    "\n",
    "    # Step 2: Chunk the text using RecursiveCharacterTextSplitter\n",
    "    print(\"Splitting text into chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  # Adjusted for meaningful chunks\n",
    "        chunk_overlap=100,  # Overlap to maintain context\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunked_text = text_splitter.split_text(pdf_text)\n",
    "    chunked_text = [chunk for chunk in chunked_text if len(chunk.strip()) > 50]\n",
    "    print(f\"Created {len(chunked_text)} chunks.\")\n",
    "\n",
    "    # Step 3: Check if collection exists, if not create it\n",
    "    try:\n",
    "        print(\"Checking if collection exists...\")\n",
    "        db = load_chroma_collection(path=db_path, name=collection_name)\n",
    "        print(f\"Collection [{collection_name}] found. Updating embeddings...\")\n",
    "    except Exception as e:\n",
    "        # If collection doesn't exist, create it\n",
    "        print(f\"Collection not found. Creating new ChromaDB collection...\")\n",
    "        db, name = create_chroma_db(documents=chunked_text, path=db_path, name=collection_name)\n",
    "        print(f\"Created new collection: {name}\")\n",
    "        # Skip the remaining steps since we've already added the documents\n",
    "        return\n",
    "\n",
    "    # Step 4: Delete all existing documents in the collection\n",
    "    print(\"Deleting existing embeddings...\")\n",
    "    all_ids = db.get()['ids']\n",
    "    if all_ids:\n",
    "        db.delete(ids=all_ids)\n",
    "        print(f\"Deleted {len(all_ids)} existing documents from the collection.\")\n",
    "    \n",
    "    # Step 5: Add new documents to the collection\n",
    "    print(\"Adding new embeddings...\")\n",
    "    for i, chunk in enumerate(chunked_text):\n",
    "        db.add(documents=[chunk], ids=str(i))\n",
    "    print(f\"Added {len(chunked_text)} new document chunks.\")\n",
    "\n",
    "    # Step 6: Query the system\n",
    "    test_query = \" What are the deliverybles that he got?\"\n",
    "    print(f\"\\nQuerying: '{test_query}'\")\n",
    "    answer = generate_answer(db, test_query)\n",
    "    print(\"Answer:\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF...\n",
      "Splitting text into chunks...\n",
      "Created 9 chunks.\n",
      "Checking if collection exists...\n",
      "Collection [rag_experiment_v5] found. Updating embeddings...\n",
      "Deleting existing embeddings...\n",
      "Deleted 143 existing documents from the collection.\n",
      "Adding new embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.premarathna\\AppData\\Local\\Temp\\ipykernel_47340\\1360047522.py:13: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
      "  db = chroma_client.get_collection(name=name, embedding_function=GeminiEmbeddingFunction())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 9 new document chunks.\n",
      "\n",
      "Querying: ' What are the deliverybles that he got?'\n",
      "Answer:\n",
      "Based on the document you provided, it looks like Akila Nishan Premarathna, who is the consultant in this \"Amendment to Consultancy Contract,\" has a specific task or deliverable mentioned.\n",
      "\n",
      "The passage states one clear deliverable:\n",
      "1.  **\"Validate the model outputs and retrain the model using new wastewater treatment plants for Mexico to keep the model up to date.\"** This particular task has a due date of 15 June 2025.\n",
      "\n",
      "The document also mentions \"Provided (Annexure 1.A) is not carried out, the fee will be deducted in proportion.\" This strongly suggests that \"Annexure 1.A\" contains a more detailed list of the deliverables. However, the actual content of Annexure 1.A isn't included in the text you've shared, so I don't have enough info to provide a full list of all the deliverables he got.\n",
      "\n",
      "Additionally, as part of his responsibilities, the consultant must make any \"modifications/improvements to the final deliverable/s without any additional fees, if requested by IWMI representative within three months after expiry of this Consultancy Contract.\" So, ensuring the final products are satisfactory and making necessary adjustments is also part of his commitment.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".vemv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
